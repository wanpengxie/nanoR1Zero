# nanoR1Zero

最小版本复现deepseek R1 Zero
1. Base model： qwen-1.5b-instruct
2. Prompt: 8k math problems
3. Eval: Math 500

春节期间复现了“纳米”版本的DeepSeek R1-Zero算法，500行代码、两张A100显卡就能体验到纯粹强化学习的魅力。在跑了上百次实验后终于看到了稳定的reward提升，相比于只看R1论文的震撼，有了更多的切身体验

1. Deepseek的GRPO和Rule Reward方案非常优雅：相比于PPO和过程监督，尤其是基于MCTS的过程监督方案（比如rStar-Math），计算过程简化到了极致，但效果很棒，即使在1.5B的小模型和5k的数据量下，也能看到非常漂亮的优化曲线，这样的方案更有潜力Scale到超大规模
   
2. 效果的稳定提升来自于序列的变长：这也在R1 tech report里提到了，在观察训练的细节时，可以发现核心是KL散度的控制，崩掉的训练基本就是KL崩掉了。从优化目标的角度来看，模型又要优化策略，又得保持Pretrain模型的分布，那么只剩下一条路可走了：变长变长变长

3. 非常期待应用层的GRPO+RLAIF方案了：GRPO的简单稳定训练，RL的可行性验证，结合Anthropic的Constitute AI，一方面解决了Prompt工程和Agent workflow都无法端到端优化的问题，另一方面也终于可以真正意义上构建数据闭环从而形成业务壁垒了。当然，RL的超强灵活性，也终于能够替代掉如同嚼蜡一般的GPT蒸馏味了

4. 几个问题：
   1) 最大的问题：更多的显卡，求A100、H100！！！
   2) GRPO不是魔法，它本质上依赖于sampling出解决方案，而不是Alpha-Zero一样搜索到解决方案，那么意味着对Pretrain模型的要求很高，这也是为什么超大模型效果才更好。但GRPO依赖于Sampling，那么对难题的解决能力并不确定，只有寄希望于课程学习带来的推理能力泛化了。
   3) Reward提升主要来自于Long CoT的产生，那么模型需要具备很好的超长输出的能力，当前也只有超大模型具备Long Context和超长输出的能力，但对于数学、Code等定制化的小模型，通过特定提升Long Context和输出能力，是否也能够提升强化学习的效果呢？
   4) RL过程中，最大的成本变成了序列生成，尤其是往Long CoT迭代过程，样本生成逐渐变成了瓶颈。那么Pretrain级别的大规模RL，只能来自于实际应用过程中的数据积累，或者提升数据生成的效率了，这一点上，DeepSeek V3作为底座的廉价成本优势就很大